# ç®€åŒ–ç‰ˆç½‘é¡µæ•°æ®ç›‘æ§æ¡†æ¶
è¿™æ˜¯ä¸€ä¸ªä¸ºç›‘æ§ç‰¹å®šç½‘ç«™æ•°æ®æ›´æ–°è€Œè®¾è®¡çš„è½»é‡çº§çˆ¬è™«æ¡†æ¶ã€‚æœ¬æ¡†æ¶åŒ…å«ä¸¤å¤§æ ¸å¿ƒç»„ä»¶ï¼šæ•°æ®é‡‡é›†æ¨¡æ¿å’Œä»»åŠ¡è°ƒé…æ¨¡æ¿ã€‚

ğŸ•·ï¸ æ•°æ®é‡‡é›†æ¨¡æ¿ï¼šè´Ÿè´£è®°å½•å¹¶è¿”å›ç½‘ç«™æ•°æ®çš„æ›´æ–°ä¿¡æ¯ã€‚åœ¨~/task/ä¸­åˆ›å»ºä»»åŠ¡è·¯å¾„å’Œçˆ¬è™«ã€‚

â° ä»»åŠ¡è°ƒé…æ¨¡æ¿ï¼šä»»åŠ¡å®‰æ’åœ¨schedule.jsonæ–‡ä»¶ä¸­ï¼Œé…åˆcronè¡¨è¾¾å¼å®‰æ’å’Œç®¡ç†çˆ¬è™«ä»»åŠ¡ã€‚è¦æ–°å¢ä¸€ä¸ªçˆ¬è™«ä»»åŠ¡ï¼Œåªéœ€åœ¨schedule.jsonä¸­å¢åŠ ä¸€ä¸ªåŒ…å«ä»»åŠ¡è·¯å¾„ä¸cronè¡¨è¾¾å¼çš„åˆ—è¡¨é¡¹å³å¯ã€‚

ğŸ” ä¸ºä»€ä¹ˆä¸ç”¨Scrapyæ¡†æ¶ï¼šåœ¨ç¼–å†™æœ¬æ¡†æ¶æ—¶ï¼Œæˆ‘è¿˜æœªæ¥è§¦è¿‡è‘—åçš„Scrapyæ¡†æ¶ã€‚æœ¬é¡¹ç›®ä¸Scrapyæœ‰å°‘é‡ç›¸ä¼¼ä¹‹å¤„ï¼Œé€‚ç”¨äºè½»é‡çº§è§£å†³æ–¹æ¡ˆçš„åœºæ™¯ï¼Œæˆ–è€…åŸºç¡€çš„çˆ¬è™«å­¦ä¹ ã€‚å®ƒåœ¨åŠŸèƒ½å’Œå®Œå–„åº¦ä¸Šè¿˜æ— æ³•ä¸Scrapyç›¸æå¹¶è®ºï¼Œå»ºè®®ç›´æ¥å­¦ä¹ scrapy...

æ¬¢è¿æœ‰å…´è¶£çš„æœ‹å‹å°è¯•ä½¿ç”¨

# Customed Web Data Monitoring Framework
This is a lightweight web scraping framework designed for monitoring data updates on specific websites. This framework is comprised of two core components: a data collection template and a task scheduling template.

ğŸ•·ï¸ Data Collection Template: Responsible for recording and returning updates on website data.

â° Task Scheduling Template: Tasks are logged in the schedule.json file, working in tandem with cron expressions to organize and manage web scraping tasks.

ğŸ” Why not Scrapy: When I was developing this framework, I hadn't yet explored the well-known Scrapy framework. This project shares some similarities with Scrapy and is suitable for lightweight solutions or basic web scraping learning. However, in terms of features and robustness, it cannot compete with Scrapy. For those looking to delve deeper into web scraping, I recommend learning Scrapy directly...

Welcome to try and explore!
